{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Fitting 2 - Automatically Figuring out Degree of Polynomial\n",
    "\n",
    "In Lesson 2, we wrote a function that could fit a polynomial of a given degree to a set of points.\n",
    "\n",
    "The downside of this function, however, is that it requires us to specify the degree of the polynomial.\n",
    "\n",
    "How can we make the model figure out the best degree of the polynomial itself?\n",
    "\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Read section 3 \"Probabilistic Interpretation\" in http://cs229.stanford.edu/notes/cs229-notes1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 'Lesson2.ipynb' # Run this to \"import\" the code from your Lesson 2 solution. The %%capture suppresses the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types\n",
    "try:\n",
    "    # Verify that import worked.\n",
    "    assert(type(fit_linear_regression) == types.FunctionType)\n",
    "    assert(type(to_feature_vector) == types.FunctionType)\n",
    "    assert(type(random_polynomial) == types.FunctionType)\n",
    "    print(\"Import worked!\")\n",
    "except:\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Failed to import Lesson2!\n",
    "        Remove the %%capture from the previous cell and run again to see error\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for you.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "def to_design_matrix(x_values, degree):\n",
    "    \"\"\"\n",
    "    Given an array of real numbers, convert them into feature\n",
    "    vectors and stack them as row vectors to form a design matrix.\n",
    "    \"\"\"\n",
    "    return np.array([to_feature_vector(x, degree) for x in x_values])\n",
    "\n",
    "def evaluate_best_degree_pickers(picker_1, picker_2=None, show_charts=True):\n",
    "    \"\"\"\n",
    "    Given a polynomial and some points sampled from it, fit a linear regression\n",
    "    model whose degree is outputted by the degree_picker(x_values_validation, y_values_validation)\n",
    "    function.\n",
    "    \n",
    "    Display the actual and predicted polynomial along with plots of the points\n",
    "    and the predicted polynomial.\n",
    "    \n",
    "    Return the absolute difference between actual polynomial degree and predicted polynomial degree.\n",
    "        If picker_2 is not None, return a pair of absolute differences.\n",
    "    \"\"\"\n",
    "    params = dict(\n",
    "        noise_sigma=0.001, \n",
    "        min_x=-100,\n",
    "        max_x=100,\n",
    "        num_points=2000,\n",
    "    )\n",
    "    polynomial, x_values, y_values = generate_dataset(**params)\n",
    "    pickers = [picker_1]\n",
    "    if picker_2 is not None:\n",
    "        pickers.append(picker_2)\n",
    "    best_degrees = [picker(x_values, y_values) for picker in pickers]\n",
    "    \n",
    "    params['polynomial'] = polynomial\n",
    "    polynomial, x_values, y_values = generate_dataset(**params)\n",
    "    \n",
    "    picker_errors = [abs((d + 1) - len(polynomial)) for d in best_degrees]\n",
    "    \n",
    "    if show_charts:\n",
    "        if picker_2 is not None:\n",
    "            print(\"HERE IS YOUR NAIVE DEGREE PICKER\")\n",
    "        evaluate_linear_regression(polynomial, x_values, y_values, best_degrees[0])\n",
    "\n",
    "        if picker_2 is not None:\n",
    "            print(\"HERE IS YOUR SMART DEGREE PICKER\")\n",
    "            evaluate_linear_regression(polynomial, x_values, y_values, best_degrees[1])\n",
    "    \n",
    "    return tuple(picker_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 1: Log Likelihood.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def compute_log_likelihood(X, y, theta, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Given a design matrix X of dimension m x d, labels of dimension m x 1,\n",
    "    and a linear regression coefficient vector dimension d x 1, compute\n",
    "    the log likelihood of the examples in the design matrix assuming the\n",
    "    zero-mean Gaussian noise model described in the notes. The sigma\n",
    "    value is provided as an argument.\n",
    "    \n",
    "    Logarithm is natural logarithm here.\n",
    "    \n",
    "    Some links that may help:\n",
    "        https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.log.html\n",
    "        https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.sqrt.html\n",
    "        https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ndarray.shape.html\n",
    "        https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.matmul.html\n",
    "        https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.subtract.html\n",
    "        https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.power.html\n",
    "        https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.sum.html\n",
    "        \n",
    "   \n",
    "    Do NOT implement this by computing the likelihood and taking the logarithm\n",
    "    at the end. You need to compute the log-likelihood directly. The reason\n",
    "    we do this is because the likelihood function involves multiplication\n",
    "    of many numbers. If those numbers are small, we will get underflow (i.e. \n",
    "    the product will be really small and the computer will round it to zero because\n",
    "    it can't represent numbers that small). If those numbers are large, we will\n",
    "    get overflow. Since the log-likelihood uses addition it will not underflow and since\n",
    "    applies a logarithm to each term, the summands will be small and the sum won't overflow.\n",
    "    \"\"\"\n",
    "    raise Exception(\"You need to implement this!\")\n",
    "\n",
    "\n",
    "# Let's test your log likelihood implementation.\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "])\n",
    "y = np.array([\n",
    "    [20],\n",
    "    [32],\n",
    "])\n",
    "theta = np.array([\n",
    "    [10],\n",
    "    [20],\n",
    "    [30],\n",
    "])\n",
    "expected_log_likelihood = -48673.837877066406\n",
    "if abs(compute_log_likelihood(X, y, theta) - expected_log_likelihood) < 0.001:\n",
    "    print(\"Implementation looks good!\")\n",
    "else:\n",
    "    print(\"Implementation has a bug!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking Polynomial Degree - Naive Solution\n",
    "\n",
    "Now that we have a log-likelihood function, here's one approach to select the degree of the polynomial.\n",
    "\n",
    "Let's try all the degrees between 0 and 20 and pick the one that maximizes the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 2: Naive Polynomial Degree Picker\n",
    "\"\"\"\n",
    "def naive_degree_picker(x_values, y_values, min_degree=0, max_degree=20):\n",
    "    \"\"\"\n",
    "    @param x_values - An array of real numbers representing the x values.\n",
    "    @param y_values - An array of real numbers representing the y values.\n",
    "        So, (x_values[i], y_values[i]) is a training example.\n",
    "    \n",
    "    @return The degree of the polynomial between min_degree and max_degree\n",
    "    that achieves the maximum log-likelihood on the dataset.\n",
    "    \n",
    "    Hint: Use fit_linear_regression, compute_log_likelihood, and to_design_matrix\n",
    "    \"\"\"\n",
    "    raise Exception(\"You need to implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 3: Evaluate Naive Degree Picker.\n",
    "\n",
    "Run this cell several times and look at the predicted polynomial vs. the actual\n",
    "polynomial. How does the degree of your predicted polynomial compare to the degree of the actual polynomial?\n",
    "\"\"\"\n",
    "\n",
    "evaluate_best_degree_pickers(naive_degree_picker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On average, let's see how good the naive solution is at getting the correct degree.\n",
    "\n",
    "Note that this will be slow because it is training lots of regression models.\n",
    "\"\"\"\n",
    "N = 100\n",
    "differences = [\n",
    "    evaluate_best_degree_pickers(naive_degree_picker, picker_2=None, show_charts=False)[0] for i in range(N)\n",
    "]\n",
    "print(\n",
    "    \"\"\"\n",
    "    On average, the absolute difference between the predicted degree and actual degree is {}\n",
    "    \"\"\".format(sum(differences)/len(differences))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smarter Solution: Validation Set\n",
    "\n",
    "The problem with our naive solution is that it fits the data WAY too closely. This is a problem in machine learning called **overfitting**. Basically, the model pays too much attention to the dataset and ends up fitting itself to both the real patterns in the data AND the noise in the data. We want a way to prevent our model from fitting the noise.\n",
    "\n",
    "Here's one way we can do that.\n",
    "\n",
    "Take our `(x, y)` pairs and randomly split them into two groups. The first group, called the training set, will have $P\\%$ of the examples. The second group, called the validation set, will have the remaining $(100 - P)\\%$ of the examples.\n",
    "\n",
    "Now, here's what we will do for each degree value between 0 and 20.\n",
    "\n",
    "1. Fit a linear regression model on the training set\n",
    "2. Compute the log likelihood on the validation set.\n",
    "3. Note down the (degree, log likelihood) pair.\n",
    "\n",
    "Then, we select the degree that has the maximum validation log likelihood.\n",
    "\n",
    "In practice $P$ is typically around 70% to 90%. In our case, we can set it much smaller because there's not a ton of noise in our dataset.\n",
    "\n",
    "### Why Does This Work?\n",
    "\n",
    "The key insight is that the model is validated on a set of examples DIFFERENT from the set it was trained on. Thus, if the model learns to fit noise in the data, then it will do poorly on the validation set because the noise will be totally different (noise is random, after all). However, if the model learns to fit useful patterns, then it will do well on the validation set because the same useful patterns will appear there too. In other words, we are forcing the model to do well on the training set, but not so closely that its learning cannot **generalize** to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 4: Split Dataset.\n",
    "\"\"\"\n",
    "import random\n",
    "def split_dataset(x_y_pairs, split_fraction):\n",
    "    \"\"\"\n",
    "    Given a set of (x, y) pairs, randomly put split_fraction fraction of them\n",
    "    into x_y_train and the remainder into x_y_validation. Return the pair\n",
    "    (x_y_train, x_y_validation).\n",
    "    \"\"\"\n",
    "    raise Exception(\"You need to implement this!\")\n",
    "\n",
    "pairs = [(i, i**2) for i in range(1, 11)]\n",
    "x_y_train, x_y_validation = split_dataset(pairs, 0.7)\n",
    "\n",
    "print(\"Sampled these: \", x_y_train, x_y_validation)\n",
    "\n",
    "\n",
    "# Ensure that the sizes are correct.\n",
    "assert(len(x_y_train) == 7)\n",
    "assert(len(x_y_validation) == 3)\n",
    "\n",
    "# Ensure that every item is a pair.\n",
    "assert(len([p for p in x_y_train if len(p) != 2]) == 0)\n",
    "assert(len([p for p in x_y_validation if len(p) != 2]) == 0)\n",
    "\n",
    "print(\"Everything looks good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 5: Smarter Polynomial Degree Picker\n",
    "\"\"\"\n",
    "\n",
    "def smart_degree_picker(x_values, y_values, min_degree=0, max_degree=20):\n",
    "    \"\"\"\n",
    "    @param x_values - An array of real numbers representing the x values.\n",
    "    @param y_values - An array of real numbers representing the y values.\n",
    "        So, (x_values[i], y_values[i]) is a training example.\n",
    "    \n",
    "    @return The degree of the polynomial between min_degree and max_degree\n",
    "    that achieves the best log likelihood on the validation set.\n",
    "    \n",
    "    Hint: Use fit_linear_regression, compute_log_likelihood, and to_design_matrix\n",
    "    Hint: Use list(zip(x_values, y_values)) to create x_y_pairs.\n",
    "    Hint: Use list(zip(*x_y_pairs)) to split x_y_pairs apart into two lists.\n",
    "    \"\"\"\n",
    "    raise Exception(\"You need to implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 6: Evaluate Smart Degree Picker.\n",
    "\n",
    "Run this cell several times and look at the predicted polynomial vs. the actual\n",
    "polynomial. How well does it fit the data? How does it compare to the Naive picker?\n",
    "\"\"\"\n",
    "evaluate_best_degree_pickers(naive_degree_picker, smart_degree_picker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On average, let's see how good the smarter solution is at getting the correct degree.\n",
    "\n",
    "Note that this will be slow because it is training lots of regression models.\n",
    "\"\"\"\n",
    "N = 100\n",
    "differences = [\n",
    "    evaluate_best_degree_pickers(naive_degree_picker, picker_2=smart_degree_picker, show_charts=False)\n",
    "    for i in range(N)\n",
    "]\n",
    "naive_differences, smart_differences = list(zip(*differences))\n",
    "print(\n",
    "    \"\"\"\n",
    "    NAIVE: On average, the absolute difference between the predicted degree and actual degree is {}\n",
    "    \"\"\".format(sum(naive_differences)/len(naive_differences))\n",
    ")\n",
    "print(\n",
    "    \"\"\"\n",
    "    SMART: On average, the absolute difference between the predicted degree and actual degree is {}\n",
    "    \"\"\".format(sum(smart_differences)/len(smart_differences))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
