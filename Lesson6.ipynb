{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Income From Census Information\n",
    "\n",
    "In this lesson, we will build a machine learning model on some real data.\n",
    "\n",
    "We will solve the following problem:\n",
    "\n",
    "Based on a person's demographic information, can you predict whether they make more or less than 50K a year?\n",
    "\n",
    "We will use this dataset: https://archive.ics.uci.edu/ml/datasets/Census+Income\n",
    "\n",
    "It contains information like a person's age, country, education, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Data\n",
    "\n",
    "The website provides TWO sets of data. The first, called `adult.data` is what we will use as our training and validation sets.\n",
    "\n",
    "The second, called `adult.test` is what we will use to evaluate the performance of our machine learning model.\n",
    "\n",
    "\n",
    "**Exercise 1: Why can we NOT `adult.test` as training data in addition to evaluation data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_data(url, filepath):\n",
    "    r = requests.get(url)\n",
    "    with open(filepath, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "\n",
    "# Download adult.data and adult.test into into /tmp.\n",
    "adult_data_path = '/tmp/adult.data'\n",
    "fetch_data(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "    adult_data_path,\n",
    ")\n",
    "\n",
    "adult_test_path = '/tmp/adult.test'\n",
    "fetch_data(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',\n",
    "    adult_test_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data\n",
    "\n",
    "We will read the data using a Python library called Pandas, which can be used to manipulate CSV data\n",
    "and also interoperates very nicely other Python libraries like numpy (numeric computation), scipy (scientific computation), scikit-learn (machine learning), matplotlib (plotting/visualization).\n",
    "\n",
    "Basically, we read the CSV into a tabular data structure called a **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Taken from https://archive.ics.uci.edu/ml/datasets/Census+Income\n",
    "\n",
    "age: continuous. \n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. \n",
    "fnlwgt: continuous. \n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. \n",
    "education-num: continuous. \n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. \n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. \n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. \n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. \n",
    "sex: Female, Male. \n",
    "capital-gain: continuous. \n",
    "capital-loss: continuous. \n",
    "hours-per-week: continuous. \n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "label_raw: >50K, <=50K. \n",
    "\"\"\"\n",
    "col_names = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education_num',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital_gain',\n",
    "    'capital_loss',\n",
    "    'hours_per_week',\n",
    "    'native_country',\n",
    "    'label',\n",
    "]\n",
    "\n",
    "\n",
    "adult_data_raw = pd.read_csv(adult_data_path, header=None, names=col_names)\n",
    "adult_test_raw = pd.read_csv(adult_test_path, header=None, names=col_names)\n",
    "\n",
    "adult_data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data\n",
    "\n",
    "Like all datasets, the census dataset needs some preprocessing to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df_raw):\n",
    "    # We re-write the label column so that it is 1 if the person makes > 50K and 0 if they\n",
    "    # make <= 50K.\n",
    "    LABEL_POSITIVE = '>50K'\n",
    "    df = df_raw.copy()\n",
    "    df.label = df.label.str.contains(LABEL_POSITIVE).astype(int)\n",
    "    # Let's get rid of the fnlwgt column because it is meaningless.\n",
    "    df = df.drop('fnlwgt', 1)\n",
    "    return df\n",
    "\n",
    "# The test data has junk on the first line.\n",
    "adult_test_raw_drop = adult_test_raw.drop(0, axis=0)\n",
    "\n",
    "adult_data = preprocess(adult_data_raw)\n",
    "adult_test = preprocess(adult_test_raw_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data\n",
    "\n",
    "Before doing any machine learning, let's compute some basic statistics and visualizations from\n",
    "the dataset.\n",
    "\n",
    "After running the two cells below, here are some observations that we can make:\n",
    "\n",
    "1. There are roughly 3x as many people who make <= 50K than > 50K\n",
    "2. Almost all the people in the dataset are from the United States\n",
    "3. People who work more hours per week tend to make more money\n",
    "4. White people tend to make more money than black people\n",
    "\n",
    "\n",
    "**\n",
    "Exercise 2: Run the two cells below and make 4-5 more observations like the ones above.\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's start by counting the number of examples (overall, positive, and negative)\n",
    "and number of features.\n",
    "\"\"\"\n",
    "\n",
    "(num_examples, num_features) = adult_data.shape\n",
    "(num_positive_examples, _) = (adult_data[adult_data.label == 1]).shape\n",
    "(num_negative_examples, _) = (adult_data[adult_data.label == 0]).shape\n",
    "print(\n",
    "    \"\"\"\n",
    "    Number of Examples: {}\n",
    "    Number of Features: {}\n",
    "    Number of Positive Examples: {}\n",
    "    Number of Negative Examples: {}\n",
    "    \"\"\".format(\n",
    "        num_examples,\n",
    "        num_features - 1, # The label is not a feature so subtract if off.\n",
    "        num_positive_examples,\n",
    "        num_negative_examples,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's make some bar charts and histograms to see how different features\n",
    "are related to the label.\n",
    "\"\"\"\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def histograms(df, feature_name):\n",
    "    colors = ['blue', 'green']\n",
    "    labels = ['negative', 'positive']\n",
    "    for i in (0, 1):\n",
    "        values = df[df.label == i][feature_name]\n",
    "        plt.hist(values, normed=1, facecolor=colors[i], alpha=0.3, label=labels[i])\n",
    "    \n",
    "    plt.legend(prop={'size': 10})\n",
    "\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of feature = {}'.format(feature_name))\n",
    "    plt.show()\n",
    "\n",
    "def bar_charts(df, feature_name):\n",
    "    colors = ['blue', 'green']\n",
    "    labels = ['negative', 'positive']\n",
    "    objects = sorted(df[feature_name].unique())\n",
    "    for i in (0, 1):\n",
    "        sub_df = df[df.label == i]\n",
    "        N = sub_df.shape[0]\n",
    "        values = sub_df.groupby([feature_name])[feature_name].count() / N\n",
    "        values_as_dict = {k: v for (k, v) in values.items()}\n",
    "        frequencies = [values_as_dict.get(k, 0) for k in objects]\n",
    "        y_pos = np.arange(len(objects))\n",
    "        plt.bar(y_pos, frequencies, facecolor=colors[i], align='center', alpha=0.3, label=labels[i])\n",
    "        plt.xticks(y_pos, objects, rotation=90)\n",
    "    plt.legend(prop={'size': 10})\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Bar chart of feature = {}'.format(feature_name))\n",
    "    plt.show()\n",
    "histograms(adult_data, 'age')\n",
    "bar_charts(adult_data, 'education_num')\n",
    "bar_charts(adult_data, 'marital_status')\n",
    "bar_charts(adult_data, 'occupation')\n",
    "bar_charts(adult_data, 'relationship')\n",
    "bar_charts(adult_data, 'race')\n",
    "bar_charts(adult_data, 'sex')\n",
    "histograms(adult_data, 'capital_gain')\n",
    "histograms(adult_data, 'capital_loss')\n",
    "histograms(adult_data, 'hours_per_week')\n",
    "bar_charts(adult_data, 'native_country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning\n",
    "\n",
    "Now that we've visualized the data, we want to feed it into a machine learning model.\n",
    "\n",
    "However, recall that supervised machine learning models take the form ($\\mathbf{x} \\in \\mathbb{R}^d$):\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = y\n",
    "$$\n",
    "\n",
    "Where $y \\in \\{0, 1, ..., K\\}$ for multi-classification ($K = 2$ is binary classification) and $y \\in \\mathbb{R}$ for regression.\n",
    "\n",
    "**Exercise 3: What kind of problem are we solving here: Classification (in which case, what is $K$?) or regression?**\n",
    "\n",
    "In order to do machine learning, we need a design matrix. That is, we need to turn our data into a matrix $X \\in \\mathbb{R}^{m \\times d}$ where $m$ is the number of training examples and $d$ is the dimensionality of the feature vector.\n",
    "\n",
    "The challenge, however, is that many of our feature values are not numbers. For example, one occupation in our dataset is called 'Tech-support', which is clearly not a number.\n",
    "\n",
    "For continuous features, like  age and capital gain, this is trivial because they are already numbers.\n",
    "\n",
    "For categorical features, like occupation and race, this is harder. Let's see how we can turn a categorical feature into a number.\n",
    "\n",
    "## String Indexing and One-Hot Encoding\n",
    "One approach you might take is **string indexing**. Basically, if there are $C$ categories, then we assign each category a numerical value between $1$ and $C$. For example, suppose there are three countries: \"United States\", \"China\", and \"Russia\". Then we can assign them numbers like \"United States\" = 1, \"China\" = 2, and \"Russia\" = 3.\n",
    "\n",
    "This is still not right though, because it imposes an order on a categorical feature, which doesn't make sense. Consequently, we will use **one-hot encoding**. Basically, will create $C$ features: \"is-united-states\", \"is-china\", and \"is-russia\", each of which is binary.\n",
    "\n",
    "With one-hot encoding, we can easily turn a categorical feature into a set of binary features and enable the machine learning model to learn them.\n",
    "\n",
    "**\n",
    "Exercise 3: What is the feature index for \"education= Bachelors\"? What is the feature value for index 75?\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's separate the label from the features.\n",
    "def separate_label(df, label_col='label'):\n",
    "    labels = df[label_col]\n",
    "    df = df.drop(label_col, axis=1)\n",
    "    return (df, labels)\n",
    "\n",
    "adult_data_features, adult_data_labels = separate_label(adult_data)\n",
    "adult_test_features, adult_test_labels = separate_label(adult_test)\n",
    "\n",
    "# Now, let's one-hot encode categorical features\n",
    "# DictVectorizer does one-hot encoding on all string columns, so we'll use that.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "adult_data_matrix_unscaled = vectorizer.fit_transform(adult_data_features.to_dict('records'))\n",
    "adult_test_matrix_unscaled = vectorizer.transform(adult_test_features.to_dict('records'))\n",
    "\n",
    "# This maps feature name to feature index.\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling to Zero-Mean Unit-Variance\n",
    "\n",
    "If you look at the matrix, you'll notice that there are some features with very large values. For example, the capital gains can be quite large because it is measured in dollars.\n",
    "\n",
    "Such large-valued features can mess up the gradient descent algorithm because it will take large steps in that dimension, but small steps in other dimensions.\n",
    "\n",
    "To mitigate this issue, we will normalize all the features so that they have zero mean and standard variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "adult_data_matrix = scaler.fit_transform(adult_data_matrix_unscaled)\n",
    "adult_test_matrix = scaler.transform(adult_test_matrix_unscaled)\n",
    "\n",
    "pd.DataFrame(adult_data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Finally, our data is ready for machine learning.\n",
    "\n",
    "Let's train a logistic regression model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(adult_data_matrix, adult_data_labels)\n",
    "adult_test_predictions = lr.predict(adult_test_matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    print('Accuracy = {}'.format(accuracy_score(y_true, y_pred)))\n",
    "    precision, recall = precision_score(y_true, y_pred), recall_score(y_true, y_pred)\n",
    "    print('Precision = {}'.format(precision))\n",
    "    print('Recall = {}'.format(recall))\n",
    "    print('F1 = {}'.format(2 * (precision * recall) / (precision + recall)))\n",
    "    \n",
    "evaluate_predictions(adult_test_labels, adult_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which features are the most important.\n",
    "# Important features will have high magnitude weights.\n",
    "\n",
    "feature_importance_pairs = []\n",
    "\n",
    "feature_name_for_index = {v: k for (k, v) in vectorizer.vocabulary_.items()}\n",
    "for feature_index in range(len(lr.coef_[0])):\n",
    "    feature_name = feature_name_for_index[feature_index]\n",
    "    coefficient = abs(lr.coef_[0][feature_index])\n",
    "    importance = abs(coefficient)\n",
    "    feature_importance_pairs.append((feature_name, importance))\n",
    "\n",
    "# Features, sorted by decreasing importance.\n",
    "sorted(feature_importance_pairs, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4: For four of the important features above, come up with an explanation for why they might be important. For example, if age should be an important feature because people gain more experience as they age and tend to make more money.**\n",
    "\n",
    "**Exercise 5: The education = 'Assoc-acdm' feature seems like it should have high importance because it indicates soembody's education. However, its importance is very low. Why do you think this is?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Challenge\n",
    "\n",
    "Now that we've trained a simple machine learning model and measured its performance, your goal is to train a better\n",
    "machine learning model.\n",
    "\n",
    "Here are some tricks for you to try:\n",
    "\n",
    "1. Select hyperparameters for the Logistic Regression (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "2. Select those hyperparameters with search + cross validation (http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "3. Use a more powerful classifier like Support Vector Machines (http://scikit-learn.org/stable/modules/svm.html#classification), Decision Trees (http://scikit-learn.org/stable/modules/tree.html), and Gradient Boosted Decision Trees (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html).\n",
    "4. Drop features that you don't think will be useful\n",
    "\n",
    "\n",
    "**Exercise 6: Beat the F1 score above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
